\section{Thesis Project}


The main goal of this thesis is to create a superhuman artificial agent in \href{https://Slither.io}{Slither.io}. The agent is trained inside a replicated environment of the game, and will be deployed onto the game to test itself against real players. Additionally, the performance and computing efficiency of different machine learning methods are assessed.
\\



\subsection{Why Slither?} % 4.1
To me, it seems interesting to apply machine learning to Slither. In part because the player's situation can get complex quickly. The optimal playstyle depends on the behavior of the opponent snakes. The player needs to anticipate the moves of multiple snakes. Crucial information can present itself on screen at every time-step. This can make it hard to find a good move and work out a solid plan. The performance of an agent depends on clever anticipation and dexterous movement. It will be interesting to study how the differently trained agents will handle these complex situations. The agents will hopefully be able to reason in a way that humans consider intelligent. 
\\[2.5mm]
Even though the states are often complex, the game has only few controls. The only control options are the left, right, or forward direction, and an optional speed boost. This makes it less convoluted to the AI to deduce which action causes which environment change, compared to video games with multiple buttons, trigger and joysticks. Fewer controls are better for efficient training and lower computing costs.
\\[2.5mm]
Games can have multiple possible team formation configuration. The most common are 1-vs-1 as in chess, team-vs-team as in football, or playing against the game, as in consoles like Atari. In free-for-all games like poker every player competes against each other under the same circumstances. To my knowledge, reinforcement learning is not often applied to free-for-all video games with hundreds of players in a single arena like in Slither. 
\\






\subsection{Previous Attempts}  % 4.2

There have been two notable papers where agents learn to play Slither with deep reinforcement learning. Both papers make use of deep Q-Learning, as used in the famous Atari DQN paper \cite{Atari_DQN_2013}. One of these papers, "Learning to play SLITHER.IO with deep reinforcement learning" \cite{prev:Slither_2019} includes a replay buffer and human demonstrations for pre-training on top of DQN learning. Both replay learning and imitation learning greatly improve performance. The paper concludes with an acknowledgement that the more advanced actor-critic methods may result in better performance.
\\[2.5mm]
The other paper, "Slither.io Deep Learning Bot" \cite{prev:Slihter_2017_OpenCV} is written less formally, but contains an interesting idea. This paper uses the computer vision library OpenCV to transform the standard game view into a feature image. This feature image labels objects like the player's snake, other snakes, pellets, and empty space. This transformation greatly reduces the input complexity from a colored image to a map with labeled sections.
\newpage \noindent
A notable similarity is that both papers make use of OpenAI's Gym as a training environment. This environment makes use of the platform Universe which contains hundreds of games including Slither. Gym lets the agent train online in Slither against real people. The main advantage for this is that no external environment needs to be created, which avoids many potential issues. The downside is that training occurs online in real-time over a network, which vastly increases computing time.
\\[2.5mm]
Both these papers manage to create an agent that is able to react to its environment. However, they do not come close to human performance. There is reason for optimism, since this thesis includes some potential improvements to these papers. This thesis makes use of more advanced training algorithms, like actor-critic methods. Also, the external training environment in Unity makes training more efficient, with many snakes in multiple environments training on variable speeds. Finally, these previous papers have not fully optimized the action space and reward distribution. In this thesis, the action is reduced to three keyboard presses and the reward is better suited for the goal, as is discussed in the next section.
\\





\subsection{Implementation Progress} % 4.3

The intelligent agents will be learning from experience inside a training environment. This environment is created in the game-engine Unity. The aim is to create a perfect replica of the original Slither game inside Unity, so that the learned behavior works as intended on the actual game. Unity has a machine learning package called \href{https://github.com/Unity-Technologies/ml-agents}{ML-Agents}. This package is used to simplify the development of intelligent agents. It contains an API for communication with Pythonâ€™s machine learning library \href{https://pytorch.org/}{PyTorch}. The PyTorch library is used to program the machine learning algorithms. The ML-Agents package already contains two RL algorithms, PPO \cite{PPO_2017} and SAC \cite{SAC_2018}. Imitation learning methods are also provided with the package. 


\subsubsection{Agent Design}
ML-Agents contains all the functionality needed to \href{https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Design-Agents.md}{design an agent}. The agent can be seen as an input-output system. It takes as input observations of the environment. These observations are processed by the agent to output actions, such that the total expected reward is greatest. So the action, observation, and reward systems need to be properly set-up for the agent to work.
\\[2.5mm]
Slither usually uses the mouse's cursor position and left button as inputs. The snake moves towards the cursor position and boosts if the button is pressed. Since the cursor can be in many positions, this action space is impractically large. Luckily, the snake can only move left, right, or forward, which can also be controlled by the left and right button on the keyboard in Slither. The action space is reduced to a left/right/forward action, and a boost/no boost action. So only two actions are used; the direction action of size 3 and the boost action of size 2.
\newpage \noindent
ML-Agents contains multiple ways to observe the environment. The most simple observation is the vector observations. A vector observations contain numerical values that represent features of the state, like position and score. A more visually oriented method is to observe via raycasts. Rays can be cast from the player's snake and collide with the environment. Information on this collision like location and collided object is observed. Another observation method is to use a camera combined with a CNN to process the images. This is similar to how humans see. Camera observations are not data efficient, since every pixel is processed. However, it is the only viable option discussed so far, since the browser game can only be observed with a camera. An option to reduce the input size while still taking in all necessary information is to divide the objects in the images into differently labeled regions \cite{prev:Slihter_2017_OpenCV}. 
\\[2.5mm]
It is difficult to shape the reward of Slither agents, since the end goals are not clearly defined. It is not clear if only the largest snake is 'winning', or that each shake is 'winning' or 'losing' to the extend of their ranking on the leaderboard. The goal can also be to increase the score and be as large as possible irrespective of other snakes. Both the ranking and the score points need to be included in the reward signal. The agent therefore receives a positive or negative reward signal depending on its relative position on the scoreboard. Agents are also rewarded for growing in size by collecting pellets. Using the speed boost slightly reduces the snake size, and should be penalized accordingly. The agent is penalized for dying. This penalty is larger for large snakes, since more progress is lost. 




\subsubsection{Unity Environment}

Unity games are composed of so-called GameObjects. In the Unity replica these objects are the snakes, the pellets, and the world. Each snake object has systems in place to consume pellets, grow in size, move its tail, die from a collision, and spawn into the world. The snake can be controlled with input from both an artificial agent and a real player. This control can be switched between a player and an agent in real-time. The pellet density can be modified, as well as the numbers of snakes in the world and the size of the world.
\\[2.5mm]
The current progress of the Unity environment can be seen via this \href{https://www.youtube.com/shorts/cQYFBBTx3z4}{YouTube link}. The snakes in the video are trained with PPO. This replica is instantly recognisable as the Slither game. The basics of the environment are there, and the training has been shown to work. However, there is still much to improve. The Unity implementation is not yet a perfect copy as it should be, and the snakes are not yet at superhuman or even human level.
\\[-1mm]
\subsection{Plan} % 4.4

The two goals of this thesis are to create a superhuman artificial agent for Slither and to compare the strengths and weaknesses of different machine learning methods. The plan is shaped around these goals. The global plan is to train agents inside a replica of Slither in Unity with machine learning. These agents will be deployed onto the real game to test their true performance. The plan is divided into steps in the roadmap below. These steps are somewhat chronological, but multiple steps can be worked on simultaneously. 

\subsubsection{Roadmap}

\noindent
1. The first priority is to complete the Unity replica. This needs to be perfect copy so that the learned behavior in Unity works as intended on the actual game. Additionally, it is useful to create a simpler version of the game for initial testing of the algorithms. The first agent in the replica is trained with a combination of imitation learning and PPO included in ML-Agents to have a reasonable baseline agent. 

\noindent
2. The trained Unity agents need to be able to play the original Slither.io inside a browser. Such a bridging system can be built with the computer vision library OpenCV. The game's input can be managed with a virtual camera, and the output with a virtual keyboard. Both these virtual systems are included in OpenCV.

\noindent
3. The only RL algorithms currently included in ML-Agents are the policy gradient methods PPO and SAC. More algorithms need to be implemented into the ML-Agents package to make a full comparison. First, the relatively simple 'Deep Q-Learning' algorithm is implemented. From here, increasingly difficult algorithms are added. The ultimate goal is to implement the MuZero algorithm from DeepMind, although this may be too ambitions. The algorithms are written in Python, and connected to Unity via ML-Agents.

\noindent
4. The agents are deployed onto Slither. The average and top scores in Slither are compared for each method. Also the computing efficiency of both training and implementation are analyzed.
\\[2.5mm] \noindent
The date of completion of the thesis is on 24 April 2022, and there are roughly 4 and a half months available. The plan is to finish one item each month, and have time left to wrap up the project. This time is used to finish writing the thesis. The results are published online on GitHub, to makes it easier to reproduce these results, and to simplify similar projects in the future.


\subsubsection{Possible Research}
These next few subjects might be be explored, depending on the feasibility and the time left. One idea is to use an extra vision layer where visible objects are divided into labeled regions. This is done to reduce the agent's input complexity from individual pixels to labeled regions. Also, instead of using a virtual keyboard, it may be interesting to use a real camera pointed at the screen and a small robot that controls a physical mouse. Another research possibility is to change the way of training the agent. For example, the agents may learn with evolutionary learning, where behaviors of bad performing snakes die off, while successful snakes have their behavior reproduced. Lastly, the agent's decision making is often seen as a black box, especially when neural nets are involved. It would be nice to get more insight in how the agents interpret their surroundings, to get a clearer view of their decision making.
% Curriculum learning might also be an option to improve training. In curriculum learning the difficulty of the environment is tuned to the agent's skill level. Snakes trained with different algorithms can compete against each other, to see if some algorithms are objectively better, or if the performance depends on the behavior of the opponent.






